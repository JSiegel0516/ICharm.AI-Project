import pandas as pd
import xarray as xr
from pathlib import Path
import requests
import shutil
from typing import Any, Dict

# Path to CSV describing datasets
csv_file = Path("./metadata.csv")  # replace with your actual CSV path
datasets_dir = Path("./datasets")
datasets_dir.mkdir(exist_ok=True)

# Read the CSV
df = pd.read_csv(csv_file)


def _prepare_for_zarr(ds: xr.Dataset) -> xr.Dataset:
    """
    Xarray can persist stale chunk metadata from the original NetCDF file
    (e.g. chunksizes=None) which triggers zarr's normalisation step to blow up
    with ``Expected an integer or an iterable of integers. Got None instead``.
    Normalising the encodings keeps the metadata simple before writing.
    """
    for name, var in ds.variables.items():
        encoding = var.encoding
        for key in ("chunksizes", "chunks", "preferred_chunks"):
            value = encoding.get(key)
            if value is None:
                encoding.pop(key, None)
            elif isinstance(value, (tuple, list)) and any(ch is None for ch in value):
                encoding.pop(key, None)
            elif key == "preferred_chunks":
                # Always drop preferred_chunks â€“ let zarr determine the chunks.
                encoding.pop(key, None)
    return ds


def _open_dataset(path: Path, decode_times: bool) -> xr.Dataset:
    open_kwargs: Dict[str, Any] = {"engine": "h5netcdf", "decode_times": decode_times}
    if decode_times:
        try:
            import cftime  # noqa: F401
        except ImportError:
            # Fall back to numpy datetimes; further handling happens upstream.
            print("cftime is not installed; continuing without it.")
        else:
            open_kwargs["use_cftime"] = True
    return xr.open_dataset(path, **open_kwargs)


for _, row in df.iterrows():
    if row["Stored"] != "local":
        continue  # skip non-local datasets for now

    local_zarr_path = Path(row["inputFile"])  # e.g., datasets/global_precipitation.zarr
    source_url = row["origLocation"]

    print(f"\nProcessing dataset: {row['datasetName']}")
    print(f"Zarr path: {local_zarr_path}")
    print(f"Source URL: {source_url}")

    # Skip if Zarr already exists
    if local_zarr_path.exists():
        print("Zarr store already exists, skipping...")
        continue

    # Ensure parent directory exists
    local_zarr_path.parent.mkdir(parents=True, exist_ok=True)

    # Download NetCDF file to temporary location
    temp_nc_file = local_zarr_path.with_suffix(".nc")
    print(f"Downloading to temporary file: {temp_nc_file}")

    try:
        with requests.get(source_url, stream=True, timeout=60) as r:
            r.raise_for_status()
            with open(temp_nc_file, "wb") as f:
                shutil.copyfileobj(r.raw, f)
    except Exception as e:
        print(f"Failed to download {source_url}: {e}")
        continue

    # Open NetCDF and convert to consolidated Zarr
    print("Opening NetCDF and converting to Zarr...")
    try:
        with _open_dataset(temp_nc_file, decode_times=True) as ds:
            _prepare_for_zarr(ds)
            ds.to_zarr(local_zarr_path, mode="w", consolidated=True)
        print(f"Zarr store created: {local_zarr_path}")
    except Exception as e:
        print(f"Failed to convert {temp_nc_file} to Zarr: {e}")
        # Fallback: try decode_times=False
        try:
            with _open_dataset(temp_nc_file, decode_times=False) as ds:
                _prepare_for_zarr(ds)
                ds.to_zarr(local_zarr_path, mode="w", consolidated=True)
            print(f"Zarr store created with decode_times=False: {local_zarr_path}")
        except Exception as e2:
            print(f"Failed again: {e2}")
            continue
    finally:
        if temp_nc_file.exists():
            temp_nc_file.unlink()  # remove temporary NetCDF

print("\nAll datasets processed.")
